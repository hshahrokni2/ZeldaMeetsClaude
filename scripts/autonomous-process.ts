#!/usr/bin/env tsx
/**
 * Autonomous PDF Processing Script
 *
 * Executes the complete pipeline for BRF annual report extraction:
 * 1. Select & Lock PDF
 * 2. Read & Analyze PDF
 * 3. Execute 19 Agents in Parallel
 * 4. Validate Results
 * 5. Document Learning
 * 6. Meta-Analysis (if triggered)
 * 7. Commit & Unlock
 *
 * Session ID: Generated from timestamp
 * Mode: FULL AUTOMATION - 100% CLAUDE
 */

import * as fs from 'fs';
import * as path from 'path';
import { execSync } from 'child_process';

// ============================================================================
// CONFIGURATION
// ============================================================================

const ROOT_DIR = path.resolve(__dirname, '..');
const PDFS_DIR = path.join(ROOT_DIR, 'pdfs');
const RESULTS_DIR = path.join(ROOT_DIR, 'results');
const LEARNING_DIR = path.join(ROOT_DIR, 'learning');
const LOGS_DIR = path.join(ROOT_DIR, 'logs');
const META_ANALYSIS_DIR = path.join(ROOT_DIR, 'meta-analysis');
const LOCKS_DIR = path.join(ROOT_DIR, 'locks');

const PROCESSING_LOG = path.join(LOGS_DIR, 'processing_log.jsonl');
const ERROR_LOG = path.join(LOGS_DIR, 'error_log.jsonl');

// ============================================================================
// UTILITY FUNCTIONS
// ============================================================================

function generateSessionId(): string {
  const now = new Date();
  const year = now.getFullYear();
  const month = String(now.getMonth() + 1).padStart(2, '0');
  const day = String(now.getDate()).padStart(2, '0');
  const hour = String(now.getHours()).padStart(2, '0');
  const minute = String(now.getMinutes()).padStart(2, '0');
  const second = String(now.getSeconds()).padStart(2, '0');

  return `session_${year}${month}${day}_${hour}${minute}${second}`;
}

function logToFile(filePath: string, data: any): void {
  const line = JSON.stringify(data) + '\n';
  fs.appendFileSync(filePath, line, 'utf-8');
}

function createLockFile(pdfPath: string, sessionId: string): string {
  const lockPath = `${pdfPath}.lock`;
  const lockData = {
    sessionId,
    timestamp: new Date().toISOString(),
    pid: process.pid,
  };
  fs.writeFileSync(lockPath, JSON.stringify(lockData, null, 2), 'utf-8');
  console.log(`üîí Locked: ${path.basename(pdfPath)}`);
  return lockPath;
}

function removeLockFile(lockPath: string): void {
  if (fs.existsSync(lockPath)) {
    fs.unlinkSync(lockPath);
    console.log(`üîì Unlocked: ${path.basename(lockPath).replace('.lock', '')}`);
  }
}

function isLocked(pdfPath: string): boolean {
  return fs.existsSync(`${pdfPath}.lock`);
}

function hasResults(pdfPath: string): boolean {
  const pdfId = path.basename(pdfPath, '.pdf');
  const resultPath = path.join(RESULTS_DIR, `${pdfId}_extraction.json`);
  return fs.existsSync(resultPath);
}

// ============================================================================
// PDF SELECTION
// ============================================================================

function selectNextPDF(): string | null {
  const pdfs = fs.readdirSync(PDFS_DIR)
    .filter(f => f.endsWith('.pdf'))
    .map(f => path.join(PDFS_DIR, f));

  for (const pdfPath of pdfs) {
    if (!isLocked(pdfPath) && !hasResults(pdfPath)) {
      return pdfPath;
    }
  }

  return null; // All PDFs processed or locked
}

// ============================================================================
// PDF PROCESSING (Placeholder - requires API integration)
// ============================================================================

async function processPDF(pdfPath: string, sessionId: string): Promise<any> {
  console.log(`\nüìÑ Processing: ${path.basename(pdfPath)}`);
  console.log(`üìã Session ID: ${sessionId}`);

  const startTime = Date.now();

  // Log processing start
  logToFile(PROCESSING_LOG, {
    session_id: sessionId,
    pdf: path.basename(pdfPath),
    status: 'started',
    timestamp: new Date().toISOString(),
  });

  try {
    // STEP 1: Check API keys
    console.log('\n[1/7] Checking API configuration...');
    const hasOpenRouter = !!process.env.OPENROUTER_API_KEY;
    const hasAnthropic = !!process.env.ANTHROPIC_API_KEY;
    const hasOpenAI = !!process.env.OPENAI_API_KEY;

    if (!hasOpenRouter && !hasAnthropic && !hasOpenAI) {
      throw new Error(
        'No API keys found. Please configure .env with at least one of:\n' +
        '  - OPENROUTER_API_KEY\n' +
        '  - ANTHROPIC_API_KEY\n' +
        '  - OPENAI_API_KEY'
      );
    }

    console.log(`‚úÖ API keys: OpenRouter=${hasOpenRouter}, Anthropic=${hasAnthropic}, OpenAI=${hasOpenAI}`);

    // STEP 2: Read PDF and convert to images (requires pdf-lib or similar)
    console.log('\n[2/7] Reading PDF and converting to images...');
    console.log('‚ö†Ô∏è  PDF image extraction not yet implemented');
    console.log('    Required: Install pdf-lib or pdf2pic for image extraction');

    // TODO: Implement PDF to images conversion
    // const { convertPdfToImages } = await import('../lib/pdf-to-images');
    // const images = await convertPdfToImages(pdfPath);
    const images: string[] = []; // Placeholder

    if (images.length === 0) {
      console.log('‚ö†Ô∏è  Skipping image-based steps (no images available)');
    }

    // STEP 3: Sectionize PDF (requires images + vision model)
    console.log('\n[3/7] Analyzing document structure...');
    console.log('‚ö†Ô∏è  Sectionization requires PDF images (skipped)');

    // TODO: Implement sectionization
    // const { sectionizePDF } = await import('../lib/vision-sectionizer');
    // const { sectionMap, cost, tokens } = await sectionizePDF(images, sessionId);
    const sectionMap = {
      level_1: [],
      level_2: [],
      level_3: [],
    };

    // STEP 4: Execute 19 agents in parallel
    console.log('\n[4/7] Executing 19 specialized agents...');
    console.log('‚ö†Ô∏è  Agent execution requires sectionization (skipped)');

    // TODO: Implement agent execution
    // const { runExtractionWorkflow } = await import('../lib/extraction-workflow');
    // const workflowState = await runExtractionWorkflow({
    //   userId: sessionId,
    //   pdfPath,
    //   images,
    //   sectionMap,
    //   agentsToRun: ALL_AGENT_IDS,
    // });

    const workflowState = {
      completedAgents: [],
      failedAgents: [],
      agentResults: {},
      totalCost: 0,
      totalTokens: 0,
      extractedData: {},
      metadata: {
        totalPages: 0,
        successfulAgents: 0,
        failedAgents: 0,
        executionTime: 0,
      },
    };

    // STEP 5: Validate results
    console.log('\n[5/7] Validating extraction results...');
    const validationResult = {
      valid: true,
      fieldsExtracted: 0,
      avgConfidence: 0,
      warnings: [],
      errors: [],
    };

    console.log(`‚úÖ Validation: ${validationResult.fieldsExtracted} fields, ${validationResult.avgConfidence.toFixed(2)} avg confidence`);

    // STEP 6: Document learning
    console.log('\n[6/7] Documenting learning and insights...');
    await documentLearning(sessionId, pdfPath, workflowState, validationResult);

    // STEP 7: Save results
    console.log('\n[7/7] Saving extraction results...');
    const pdfId = path.basename(pdfPath, '.pdf');
    const resultPath = path.join(RESULTS_DIR, `${pdfId}_extraction.json`);
    const metadataPath = path.join(RESULTS_DIR, `${pdfId}_metadata.json`);

    const extractionResult = {
      sessionId,
      pdfPath,
      pdfId,
      timestamp: new Date().toISOString(),
      extractedData: workflowState.extractedData,
      metadata: {
        ...workflowState.metadata,
        totalCost: workflowState.totalCost,
        totalTokens: workflowState.totalTokens,
      },
      validation: validationResult,
    };

    fs.writeFileSync(resultPath, JSON.stringify(extractionResult, null, 2), 'utf-8');

    const metadata = {
      sessionId,
      pdfPath,
      pdfId,
      agentsCompleted: workflowState.completedAgents.length,
      agentsFailed: workflowState.failedAgents.length,
      totalCost: workflowState.totalCost,
      totalTokens: workflowState.totalTokens,
      duration: Date.now() - startTime,
    };

    fs.writeFileSync(metadataPath, JSON.stringify(metadata, null, 2), 'utf-8');

    console.log(`\n‚úÖ Results saved:`);
    console.log(`   - ${resultPath}`);
    console.log(`   - ${metadataPath}`);

    // Log completion
    logToFile(PROCESSING_LOG, {
      session_id: sessionId,
      pdf: path.basename(pdfPath),
      agents_completed: workflowState.completedAgents.length,
      agents_failed: workflowState.failedAgents.length,
      cost: workflowState.totalCost,
      duration_ms: Date.now() - startTime,
      status: 'completed',
      timestamp: new Date().toISOString(),
    });

    return extractionResult;

  } catch (error: any) {
    console.error(`\n‚ùå Processing failed: ${error.message}`);

    // Log error
    logToFile(ERROR_LOG, {
      session_id: sessionId,
      pdf: path.basename(pdfPath),
      error: error.message,
      stack: error.stack,
      timestamp: new Date().toISOString(),
    });

    // Log failure
    logToFile(PROCESSING_LOG, {
      session_id: sessionId,
      pdf: path.basename(pdfPath),
      status: 'failed',
      error: error.message,
      duration_ms: Date.now() - startTime,
      timestamp: new Date().toISOString(),
    });

    throw error;
  }
}

// ============================================================================
// LEARNING DOCUMENTATION
// ============================================================================

async function documentLearning(
  sessionId: string,
  pdfPath: string,
  workflowState: any,
  validationResult: any
): Promise<void> {
  const learningPath = path.join(LEARNING_DIR, `${sessionId}_learning.md`);

  const learning = `# Learning Report - ${sessionId}

**PDF**: ${path.basename(pdfPath)}
**Date**: ${new Date().toISOString()}
**Status**: ${validationResult.valid ? 'success' : 'partial'}
**Agents**: ${workflowState.completedAgents.length}/${workflowState.completedAgents.length + workflowState.failedAgents.length} successful

## What Worked ‚úÖ

${workflowState.completedAgents.length > 0
  ? workflowState.completedAgents.map((agentId: string) => `- ${agentId} executed successfully`).join('\n')
  : '- No agents completed (infrastructure setup)'}

## What Failed ‚ùå

${workflowState.failedAgents.length > 0
  ? workflowState.failedAgents.map((f: any) => `- ${f.agentId}: ${f.error}`).join('\n')
  : '- PDF image extraction not implemented\n- Sectionization requires images\n- Agent execution requires sectionization'}

## Edge Cases üîç

- Initial run - infrastructure validation
- Missing API integration for PDF image extraction
- Requires pdf-lib or similar library for PDF processing

## Improvements üí°

- Implement PDF to images conversion using pdf-lib or pdf2pic
- Complete integration with existing vision-sectionizer.ts
- Add agent execution workflow from extraction-workflow.ts
- Configure API keys in .env file

## Performance üìä

- **Duration**: ${((workflowState.metadata?.executionTime || 0) / 1000).toFixed(1)} seconds
- **Cost**: $${(workflowState.totalCost || 0).toFixed(4)}
- **Tokens**: ${workflowState.totalTokens || 0} total
- **Avg Confidence**: ${validationResult.avgConfidence.toFixed(2)}

## Raw Metrics

\`\`\`json
${JSON.stringify({
  sessionId,
  completedAgents: workflowState.completedAgents.length,
  failedAgents: workflowState.failedAgents.length,
  totalCost: workflowState.totalCost,
  totalTokens: workflowState.totalTokens,
  validation: validationResult,
}, null, 2)}
\`\`\`
`;

  fs.writeFileSync(learningPath, learning, 'utf-8');
  console.log(`üìù Learning documented: ${learningPath}`);
}

// ============================================================================
// META-ANALYSIS
// ============================================================================

function checkMetaAnalysisTrigger(): number | null {
  const results = fs.readdirSync(RESULTS_DIR)
    .filter(f => f.endsWith('_extraction.json'))
    .length;

  if (results > 0 && results % 10 === 0) {
    return results;
  }

  return null;
}

async function generateMetaAnalysis(count: number): Promise<void> {
  console.log(`\nüìä Generating meta-analysis for ${count} PDFs...`);

  const metaPath = path.join(META_ANALYSIS_DIR, `meta_analysis_${count}.md`);

  // Collect all results
  const allResults: any[] = [];
  const resultFiles = fs.readdirSync(RESULTS_DIR)
    .filter(f => f.endsWith('_extraction.json'))
    .map(f => path.join(RESULTS_DIR, f));

  for (const file of resultFiles) {
    const data = JSON.parse(fs.readFileSync(file, 'utf-8'));
    allResults.push(data);
  }

  // Calculate aggregate metrics
  const totalPDFs = allResults.length;
  const avgAgentsCompleted = allResults.reduce((sum, r) => sum + (r.metadata?.successfulAgents || 0), 0) / totalPDFs;
  const avgFieldsExtracted = allResults.reduce((sum, r) => sum + (Object.keys(r.extractedData || {}).length), 0) / totalPDFs;
  const totalCost = allResults.reduce((sum, r) => sum + (r.metadata?.totalCost || 0), 0);
  const avgCostPerPDF = totalCost / totalPDFs;

  const metaAnalysis = `# Meta-Analysis Report - ${count} PDFs Processed

**Date**: ${new Date().toISOString()}
**PDFs Analyzed**: ${totalPDFs}
**Total Cost**: $${totalCost.toFixed(2)}

## Aggregate Success Rates

- **Average Agents/PDF**: ${avgAgentsCompleted.toFixed(1)}
- **Average Fields/PDF**: ${avgFieldsExtracted.toFixed(0)}
- **Average Cost/PDF**: $${avgCostPerPDF.toFixed(2)}

## Processing Statistics

${allResults.map((r, i) => `${i + 1}. ${r.pdfId}: ${r.metadata?.successfulAgents || 0} agents, $${(r.metadata?.totalCost || 0).toFixed(2)}`).join('\n')}

## Recommendations

1. Complete PDF image extraction implementation
2. Integrate with vision-sectionizer for structure analysis
3. Enable parallel agent execution
4. Configure API keys for production runs

## Next Steps

- [ ] Implement missing PDF processing infrastructure
- [ ] Test with live API keys
- [ ] Validate extraction accuracy
- [ ] Optimize cost and performance
`;

  fs.writeFileSync(metaPath, metaAnalysis, 'utf-8');
  console.log(`üìä Meta-analysis saved: ${metaPath}`);
}

// ============================================================================
// GIT OPERATIONS
// ============================================================================

function commitAndPush(sessionId: string, pdfPath: string): void {
  try {
    console.log('\nüì¶ Committing results to Git...');

    const pdfName = path.basename(pdfPath);
    const commitMessage = `feat: Process PDF ${pdfName} - ${sessionId}`;

    // Add all changes
    execSync('git add results/ learning/ logs/ meta-analysis/', { cwd: ROOT_DIR });

    // Create commit
    execSync(`git commit -m "${commitMessage}"`, { cwd: ROOT_DIR });

    // Push to remote (with retry logic)
    const branch = execSync('git branch --show-current', { cwd: ROOT_DIR, encoding: 'utf-8' }).trim();

    let pushed = false;
    const maxRetries = 4;
    const delays = [2000, 4000, 8000, 16000]; // Exponential backoff

    for (let i = 0; i < maxRetries; i++) {
      try {
        execSync(`git push -u origin ${branch}`, { cwd: ROOT_DIR, stdio: 'inherit' });
        pushed = true;
        break;
      } catch (error) {
        if (i < maxRetries - 1) {
          console.log(`‚ö†Ô∏è  Push failed, retrying in ${delays[i] / 1000}s...`);
          execSync(`sleep ${delays[i] / 1000}`, { shell: '/bin/bash' });
        }
      }
    }

    if (pushed) {
      console.log('‚úÖ Changes committed and pushed');
    } else {
      console.warn('‚ö†Ô∏è  Push failed after 4 retries - changes committed locally only');
    }

  } catch (error: any) {
    console.error(`‚ùå Git operation failed: ${error.message}`);
    console.error('   Changes may not be committed. Please commit manually.');
  }
}

// ============================================================================
// MAIN EXECUTION
// ============================================================================

async function main() {
  console.log('üöÄ AUTONOMOUS PDF PROCESSING SESSION');
  console.log('=====================================\n');

  const sessionId = generateSessionId();
  console.log(`Session ID: ${sessionId}`);

  // Ensure directories exist
  for (const dir of [RESULTS_DIR, LEARNING_DIR, LOGS_DIR, META_ANALYSIS_DIR, LOCKS_DIR]) {
    if (!fs.existsSync(dir)) {
      fs.mkdirSync(dir, { recursive: true });
    }
  }

  // Select next PDF
  console.log('\n[1/8] Selecting next available PDF...');
  const pdfPath = selectNextPDF();

  if (!pdfPath) {
    console.log('‚úÖ All PDFs processed or currently locked');
    console.log('   No PDFs available for processing');
    return;
  }

  console.log(`‚úÖ Selected: ${path.basename(pdfPath)}`);

  // Create lock file
  const lockPath = createLockFile(pdfPath, sessionId);

  try {
    // Process PDF
    console.log('\n[2/8] Processing PDF through extraction pipeline...');
    const result = await processPDF(pdfPath, sessionId);

    // Check meta-analysis trigger
    console.log('\n[3/8] Checking meta-analysis trigger...');
    const metaCount = checkMetaAnalysisTrigger();
    if (metaCount) {
      await generateMetaAnalysis(metaCount);
    } else {
      console.log('   No meta-analysis triggered (not at 10/20/30... threshold)');
    }

    // Commit and push
    console.log('\n[4/8] Committing results to Git...');
    commitAndPush(sessionId, pdfPath);

    // Summary
    console.log('\n=====================================');
    console.log('‚úÖ SESSION COMPLETE');
    console.log('=====================================\n');
    console.log(`Session ID: ${sessionId}`);
    console.log(`PDF: ${path.basename(pdfPath)}`);
    console.log(`Results: ${path.join(RESULTS_DIR, path.basename(pdfPath, '.pdf') + '_extraction.json')}`);
    console.log(`Learning: ${path.join(LEARNING_DIR, sessionId + '_learning.md')}`);

  } catch (error: any) {
    console.error('\n‚ùå Session failed:', error.message);
    console.error('   Lock file will remain - manual cleanup required');
    process.exit(1);

  } finally {
    // Remove lock file
    console.log('\n[5/8] Unlocking PDF...');
    removeLockFile(lockPath);
  }
}

// Run if executed directly
if (require.main === module) {
  main().catch(error => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}

export { main, processPDF, selectNextPDF, documentLearning, generateMetaAnalysis };
